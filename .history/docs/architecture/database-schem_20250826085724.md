# Databricks Unity Catalog: Advanced Data Governance & Security

A comprehensive guide to implementing enterprise-grade data governance using Unity Catalog's advanced features.

---

## Prerequisites

- Databricks workspace with Unity Catalog enabled
- Account admin or metastore admin privileges
- Basic SQL knowledge
- Understanding of data governance concepts

---

## Part 1: Unity Catalog Architecture

### Understanding the Hierarchy

Unity Catalog implements a three-tier namespace for data organization:

```
Metastore (Account Level)
    └── Catalog (Database Groups)
        └── Schema (Database)
            └── Tables/Views/Functions/Volumes
```

### Initial Setup

```sql
-- Check available metastore privileges
SELECT * FROM system.information_schema.metastore_privileges;

-- View available catalogs
SELECT * FROM system.information_schema.catalogs;
```

### Exercise 1: Catalog Deployment

```sql
-- Deploy a new catalog for your team
USE CATALOG 'd4001-centralus-tdvip-external_data_catalog';
USE SCHEMA 'ted-bloomberg';

-- Create volume for external data storage
DROP VOLUME IF EXISTS test_files;
CREATE VOLUME test_files;

-- Verify catalog structure
SHOW CATALOGS;
SHOW SCHEMAS IN d4001-centralus-tdvip-external_data_catalog;
```

**Key Concepts:**
- Catalogs provide isolated namespaces for different teams/projects
- Schemas organize related tables within a catalog
- Volumes manage external file storage

---

## Part 2: Access Control Implementation

### Privilege Management

Unity Catalog uses fine-grained access controls at each level:

```sql
-- View current catalog privileges
SELECT * FROM system.information_schema.catalog_privileges;

-- Grant specific privileges
SHOW GRANTS ON CATALOG `d4001-centralus-tdvip-catalogdemo1`;

-- Schema-level permissions
SHOW GRANTS ON SCHEMA `d4001-centralus-tdvip-catalogdemo1`.`schemademo1`;
```

### Exercise 2: Implement Role-Based Access

```sql
-- Catalog-level access for data engineers
GRANT USE CATALOG ON CATALOG d4001-centralus-tdvip-catalogdemo1 
TO `data-engineers@company.com`;

-- Schema-level access for analysts
GRANT SELECT ON SCHEMA d4001-centralus-tdvip-catalogdemo1.schemademo1 
TO `analysts@company.com`;

-- Table-specific access
GRANT SELECT, MODIFY ON TABLE d4001-centralus-tdvip-catalogdemo1.schemademo1.rick_uc_test_table 
TO `specific-user@company.com`;
```

**Security Best Practices:**
- Follow principle of least privilege
- Use groups instead of individual user grants
- Regularly audit permissions using system tables

---

## Part 3: Table Management Patterns

### Creating Tables with Location Control

```sql
USE CATALOG `d4001-centralus-tdvip-catalogdemo1`;
USE SCHEMA schemademo1;

-- Create managed table
CREATE TABLE IF NOT EXISTS rick_uc_test_table (
    deptcode INT,
    deptname STRING,
    location STRING
);

-- Insert sample data
INSERT INTO rick_uc_test_table VALUES
(10, 'IT Securities', 'Toronto'),
(20, 'IT Securities', 'New York City'),
(30, 'IT Securities', 'Dublin');

-- Query the table
SELECT * FROM rick_uc_test_table;
```

### Exercise 3: External Table Integration

```sql
-- Create external table pointing to cloud storage
CREATE TABLE IF NOT EXISTS raw_ingestion_external 
LOCATION 'abfss://container@storageaccount.dfs.core.windows.net/EXT_TABLE_TEST/';

-- Schema inference for external data
CREATE TABLE external_json_data
USING JSON
LOCATION 's3://your-bucket/data/';
```

---

## Part 4: Data Lineage & Relationships

### Building Connected Data Models

```sql
-- Create dimension table
CREATE TABLE IF NOT EXISTS rick_uc_joined_table (
    country_code STRING,
    country_name STRING,
    region STRING,
    gdp_billions DECIMAL(10,2)
);

-- Insert reference data
INSERT INTO rick_uc_joined_table VALUES
('US', 'United States', 'North America', 21427.7),
('CA', 'Canada', 'North America', 1988.3),
('IE', 'Ireland', 'Europe', 498.6);

-- Join tables for lineage tracking
SELECT 
    a.deptcode,
    a.deptname,
    b.country_name,
    b.gdp_billions
FROM rick_uc_test_table a
JOIN rick_uc_joined_table b
    ON a.location = b.country_name;
```

### Exercise 4: Track Data Dependencies

```sql
-- View table lineage
SELECT * FROM system.information_schema.table_lineage
WHERE table_name = 'rick_uc_joined_table';

-- Analyze column-level lineage
SELECT * FROM system.information_schema.column_lineage
WHERE table_name = 'rick_uc_joined_table';
```

---

## Part 5: User-Defined Functions (UDFs)

### Creating Reusable Business Logic

```sql
-- Create UDF for name standardization
CREATE OR REPLACE FUNCTION get_name_length(name STRING)
RETURNS INT
RETURN LENGTH(name);

-- Use the function in queries
SELECT 
    deptname,
    get_name_length(deptname) as name_length
FROM rick_uc_test_table;

-- Create complex business logic UDF
CREATE OR REPLACE FUNCTION calculate_risk_score(
    revenue DECIMAL(10,2),
    region STRING
)
RETURNS STRING
LANGUAGE SQL
AS $$
    CASE 
        WHEN revenue > 1000 AND region = 'North America' THEN 'LOW'
        WHEN revenue > 500 AND region = 'Europe' THEN 'MEDIUM'
        ELSE 'HIGH'
    END
$$;
```

### Exercise 5: Build Department Analytics UDF

```sql
-- Create analytics function
CREATE OR REPLACE FUNCTION dept_analytics(dept_code INT)
RETURNS TABLE(
    dept_name STRING,
    location_count INT,
    regions STRING
)
LANGUAGE SQL
AS $$
    SELECT 
        deptname,
        COUNT(DISTINCT location) as location_count,
        STRING_AGG(DISTINCT location, ', ') as regions
    FROM rick_uc_test_table
    WHERE deptcode = dept_code
    GROUP BY deptname
$$;

-- Use the table function
SELECT * FROM TABLE(dept_analytics(10));
```

---

## Part 6: Row-Level Security

### Implementing Dynamic Row Filtering

```sql
-- Create row filter function
CREATE OR REPLACE FUNCTION us_filter(region STRING)
RETURNS BOOLEAN
RETURN IF(IS_ACCOUNT_GROUP_MEMBER('admin'), True, region='USA');

-- Apply row filter to sensitive table
ALTER TABLE rick_uc_joined_table 
SET ROW FILTER us_filter ON (country_name);

-- Create filtered view for analysts
CREATE OR REPLACE VIEW rick_uc_joined_table_filtered AS
SELECT * FROM rick_uc_joined_table;

-- Test row filtering
SELECT * FROM rick_uc_joined_table_filtered;
```

### Exercise 6: Multi-Level Row Security

```sql
-- Complex row filter based on multiple conditions
CREATE OR REPLACE FUNCTION advanced_row_filter(
    dept_code INT,
    location STRING
)
RETURNS BOOLEAN
LANGUAGE SQL
AS $$
    CASE
        WHEN IS_ACCOUNT_GROUP_MEMBER('executives') THEN TRUE
        WHEN IS_ACCOUNT_GROUP_MEMBER('managers') THEN dept_code IN (10, 20)
        WHEN IS_ACCOUNT_GROUP_MEMBER('analysts') THEN location = CURRENT_USER_REGION()
        ELSE FALSE
    END
$$;

-- Apply to production table
ALTER TABLE production_data
SET ROW FILTER advanced_row_filter ON (department_id, office_location);
```

---

## Part 7: Column-Level Security

### Column Masking Implementation

```sql
-- Create masking function for PII
CREATE OR REPLACE FUNCTION deptname_mask(department STRING)
RETURNS STRING
RETURN IF(IS_ACCOUNT_GROUP_MEMBER('admin'), department, '******');

-- Apply column mask
ALTER TABLE rick_uc_joined_table
ALTER COLUMN deptname
SET MASK deptname_mask;

-- Test masking
SELECT * FROM rick_uc_joined_table;
```

### Exercise 7: Advanced Masking Patterns

```sql
-- Partial masking for sensitive data
CREATE OR REPLACE FUNCTION partial_mask_ssn(ssn STRING)
RETURNS STRING
LANGUAGE SQL
AS $$
    CASE
        WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin') THEN ssn
        WHEN IS_ACCOUNT_GROUP_MEMBER('hr_viewer') THEN 
            CONCAT('XXX-XX-', SUBSTRING(ssn, -4))
        ELSE 'XXX-XX-XXXX'
    END
$$;

-- Hash-based masking for analytics
CREATE OR REPLACE FUNCTION hash_mask_email(email STRING)
RETURNS STRING
RETURN IF(
    IS_ACCOUNT_GROUP_MEMBER('data_scientists'),
    email,
    SHA2(email, 256)
);
```

---

## Part 8: External Data Integration

### Working with Cloud Storage

```sql
-- Create external location
CREATE EXTERNAL LOCATION IF NOT EXISTS azure_external
URL 'abfss://container@storage.dfs.core.windows.net/'
WITH (STORAGE CREDENTIAL = azure_credential);

-- Create external table with schema
CREATE TABLE external_weather_data (
    date DATE,
    location STRING,
    temperature DOUBLE,
    humidity INT,
    conditions STRING
)
USING PARQUET
LOCATION 'abfss://container@storage.dfs.core.windows.net/weather/';

-- Query external data
SELECT 
    location,
    AVG(temperature) as avg_temp,
    AVG(humidity) as avg_humidity
FROM external_weather_data
WHERE date >= '2024-01-01'
GROUP BY location;
```

---

## Part 9: Performance Optimization

### Query Optimization Strategies

```sql
-- Analyze table statistics
ANALYZE TABLE rick_uc_test_table COMPUTE STATISTICS;

-- Create optimized view with filters
CREATE OR REPLACE VIEW optimized_dept_view AS
SELECT /*+ BROADCAST(small_table) */
    a.*,
    b.country_name
FROM rick_uc_test_table a
JOIN rick_uc_joined_table b
    ON a.location = b.country_name
WHERE a.deptcode IN (10, 20, 30);

-- Z-order optimization for frequently filtered columns
OPTIMIZE rick_uc_test_table
ZORDER BY (deptcode, location);
```

### Exercise 8: Performance Monitoring

```sql
-- Monitor table access patterns
SELECT 
    table_name,
    operation,
    user_name,
    timestamp
FROM system.access.audit
WHERE table_schema = 'schemademo1'
    AND timestamp >= current_timestamp() - INTERVAL 1 DAY
ORDER BY timestamp DESC;

-- Identify slow queries
SELECT 
    query_text,
    execution_time,
    rows_produced,
    user_name
FROM system.query.history
WHERE execution_time > 5000 -- milliseconds
ORDER BY execution_time DESC
LIMIT 10;
```

---

## Part 10: Production Deployment Checklist

### Pre-Production Validation

```python
# Python validation script for Databricks notebook
def validate_catalog_setup(catalog_name, schema_name):
    """
    Validate Unity Catalog setup before production deployment
    """
    
    # Check catalog exists
    catalogs = spark.sql("SHOW CATALOGS").collect()
    catalog_exists = any(row.catalog == catalog_name for row in catalogs)
    
    if not catalog_exists:
        raise ValueError(f"Catalog {catalog_name} not found")
    
    # Check permissions
    grants = spark.sql(f"""
        SELECT * FROM system.information_schema.table_privileges
        WHERE table_catalog = '{catalog_name}'
        AND table_schema = '{schema_name}'
    """).collect()
    
    # Validate row-level security
    row_filters = spark.sql(f"""
        SELECT table_name, row_filter
        FROM system.information_schema.row_filters
        WHERE table_catalog = '{catalog_name}'
        AND table_schema = '{schema_name}'
    """).collect()
    
    # Validate column masking
    column_masks = spark.sql(f"""
        SELECT table_name, column_name, masking_policy
        FROM system.information_schema.column_masks
        WHERE table_catalog = '{catalog_name}'
        AND table_schema = '{schema_name}'
    """).collect()
    
    return {
        "catalog_valid": catalog_exists,
        "grants_count": len(grants),
        "row_filters_count": len(row_filters),
        "column_masks_count": len(column_masks)
    }

# Run validation
results = validate_catalog_setup(
    "d4001-centralus-tdvip-catalogdemo1",
    "schemademo1"
)
print(f"Validation Results: {results}")
```

### Deployment Script

```sql
-- Production deployment template
BEGIN;

-- 1. Create catalog structure
CREATE CATALOG IF NOT EXISTS production_catalog;
USE CATALOG production_catalog;
CREATE SCHEMA IF NOT EXISTS analytics;

-- 2. Set up permissions
GRANT USE CATALOG ON CATALOG production_catalog TO `analysts@company.com`;
GRANT CREATE, SELECT ON SCHEMA production_catalog.analytics TO `data_engineers@company.com`;

-- 3. Create tables with security
CREATE TABLE IF NOT EXISTS secure_data (
    id BIGINT,
    sensitive_field STRING,
    public_field STRING
);

-- 4. Apply security policies
ALTER TABLE secure_data 
SET ROW FILTER production_row_filter ON (id);

ALTER TABLE secure_data
ALTER COLUMN sensitive_field
SET MASK production_mask;

-- 5. Create monitoring views
CREATE VIEW data_access_audit AS
SELECT * FROM system.access.audit
WHERE table_catalog = 'production_catalog';

COMMIT;
```

---

## Best Practices Summary

### Security
1. **Least Privilege**: Grant minimum necessary permissions
2. **Group-Based Access**: Use AD groups over individual users
3. **Regular Audits**: Review access logs weekly
4. **Mask PII**: Always mask sensitive columns
5. **Row Filters**: Implement for multi-tenant data

### Performance
1. **Statistics**: Run ANALYZE TABLE regularly
2. **Z-Ordering**: Use for high-cardinality filters
3. **Partitioning**: Partition by date for time-series data
4. **Caching**: Cache frequently accessed small tables
5. **Monitor**: Track slow queries and optimize

### Governance
1. **Naming Conventions**: Use consistent naming patterns
2. **Documentation**: Comment all tables and columns
3. **Lineage**: Track data dependencies
4. **Version Control**: Store DDL in Git
5. **Testing**: Validate security policies before production

---

## Troubleshooting Guide

### Common Issues

**Permission Denied**
```sql
-- Check current user permissions
SELECT * FROM system.information_schema.current_user_privileges;

-- View effective permissions on table
SHOW GRANTS ON TABLE catalog.schema.table;
```

**Row Filter Not Working**
```sql
-- Verify filter function
DESCRIBE FUNCTION filter_function;

-- Test filter logic
SELECT filter_function('test_value');

-- Check filter application
SELECT * FROM system.information_schema.row_filters
WHERE table_name = 'your_table';
```

**Performance Issues**
```sql
-- Check table statistics
DESCRIBE EXTENDED catalog.schema.table;

-- View query plan
EXPLAIN SELECT * FROM your_complex_query;

-- Force statistics refresh
ANALYZE TABLE catalog.schema.table COMPUTE STATISTICS;
```

---

## Next Steps

1. **Advanced Topics**
   - Delta Lake integration
   - Change data capture (CDC)
   - Time travel queries
   - Schema evolution

2. **Integration Patterns**
   - DBT with Unity Catalog
   - Terraform for infrastructure as code
   - CI/CD pipelines for catalog changes

3. **Compliance & Auditing**
   - GDPR compliance patterns
   - SOC2 audit preparation
   - Data retention policies

Ready to implement enterprise-grade data governance with Unity Catalog!